{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict Survival of passengers on the Titanic\n",
    "# Reference: https://ww2.amstat.org/publications/jse/v3n3/datasets.dawson.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Machine Learning Library of PySpark\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.sql import Row, SQLContext\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from numpy import array\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "# Library for confusion matrix, precision, test error\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "# Library For Area under ROC curve and Area under precision-recall curve\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "# Assign resources to the application\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The data will be loaded into an array.\n",
    "# This is the summary of the data structure, including the column position and name.\n",
    "# The first filed starts from position 0. \n",
    "\n",
    "# 0 Name    -  Passenger first and last name.\n",
    "# 1 PClass  -  Passenger class (1st, 2nd, or 3rd)\n",
    "# 2 Age\n",
    "# 3 Sex\n",
    "# 4 Survived -  1 if the passenger survived;  0 if the passenger did not survive\n",
    "# 5 PersonID\n",
    "\n",
    "# Label is a target variable.  PersonInfo is a list of independent variables besides unique identifier\n",
    "\n",
    "LabeledDocument = Row(\"PersonID\", \"PersonInfo\", \"label\")\n",
    "\n",
    "# Define a function that parses the raw CSV file and returns an object of type LabeledDocument\n",
    "\n",
    "def parseDocument(line):\n",
    "    values = [str(x) for x in line.split(',')] \n",
    "    if (values[4]>'0'):\n",
    "      alive = 1.0\n",
    "    else:\n",
    "     alive = 0.0\n",
    "        \n",
    "    textValue = str(values[1]) + \" \" + str(values[2])+\" \" + str(values[3])\n",
    "    return LabeledDocument(values[5], textValue, alive)\n",
    "\n",
    "\n",
    "# Load the raw Titanic.csv file, parse it using the function above\n",
    "# @hidden_cell\n",
    "# This function is used to setup the access of Spark to your Object Storage. The definition contains your credentials.\n",
    "# You might want to remove those credentials before you share your notebook.\n",
    "def set_hadoop_config_with_credentials_11c12758b6344651a9a656f1ba0d558d(name):\n",
    "    \"\"\"This function sets the Hadoop configuration so it is possible to\n",
    "    access data from Bluemix Object Storage using Spark\"\"\"\n",
    "\n",
    "    prefix = 'fs.swift.service.' + name\n",
    "    hconf = sc._jsc.hadoopConfiguration()\n",
    "    hconf.set(prefix + '.auth.url', 'https://identity.open.softlayer.com'+'/v3/auth/tokens')\n",
    "    hconf.set(prefix + '.auth.endpoint.prefix', 'endpoints')\n",
    "    hconf.set(prefix + '.tenant', '79ab4cb5dbe64b6c924aec2badf3fe96')\n",
    "    hconf.set(prefix + '.username', '934bcd78d58b4bc9bb0bde1b15aaa627')\n",
    "    hconf.set(prefix + '.password', 'RYuZ_*tA-.hh4j9T')\n",
    "    hconf.setInt(prefix + '.http.port', 8080)\n",
    "    hconf.set(prefix + '.region', 'dallas')\n",
    "    hconf.setBoolean(prefix + '.public', False)\n",
    "\n",
    "# you can choose any name\n",
    "name = 'keystone'\n",
    "set_hadoop_config_with_credentials_11c12758b6344651a9a656f1ba0d558d(name)\n",
    "\n",
    "data = sc.textFile(\"swift://Default.\" + name + \"/Titanic.csv\")\n",
    "data.take(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load the data into a dataframe\n",
    "documents = data.filter(lambda s: \"Name\" not in s).map(parseDocument)\n",
    "training = documents.toDF() # ToDataFrame\n",
    "training.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set up Logistic Regression using Pipeline of SparkML\n",
    "tokenizer = Tokenizer(inputCol=\"PersonInfo\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set up Logistic Regression Model\n",
    "# the stages are executed in order\n",
    "model = pipeline.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Verify the training document \n",
    "# to checkpoint your progress with the application\n",
    "training.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PersonInfo here is a combination of pclass followed by age and sex\n",
    "# *x in Document(*x) is for header of Row(\"PersonID\", \"PersonInfo\")\n",
    "Document = Row(\"PersonID\", \"PersonInfo\")\n",
    "test = sc.parallelize([(757, \"1st 20 female\"),\n",
    "              (758, \"3rd 15 male\"),\n",
    "              (759, \"2nd 16 female\"),\n",
    "              (760, \"1st 22 male\"),\n",
    "              (761, \"3rd 17 female\"),\n",
    "              (762, \"1st 7 male\")]).map(lambda x: Document(*x)).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make predictions on test documents and print columns of interest\n",
    "prediction = model.transform(test)\n",
    "selected = prediction.select(\"PersonInfo\", \"prediction\", \"probability\")\n",
    "for row in selected.collect():\n",
    "    print row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluate the Logistic Regression model\n",
    "# Select (prediction, true label) and compute test error\n",
    "pred_lr=model.transform(training).select(\"prediction\", \"label\")\n",
    "eval_lr=MulticlassClassificationEvaluator (\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"precision\")\n",
    "accuracy_lr=eval_lr.evaluate(pred_lr)\n",
    "# create RDD\n",
    "predictionAndLabels_lr=pred_lr.rdd\n",
    "metrics_lr=MulticlassMetrics(predictionAndLabels_lr)\n",
    "precision_lr=metrics_lr.precision()\n",
    "recall_lr=metrics_lr.recall()\n",
    "f1Measure_lr = metrics_lr.fMeasure()\n",
    "print(\"F1 Measure = %s\" % f1Measure_lr)\n",
    "print (\"Test Accuracy = %s\" %accuracy_lr)\n",
    "print (\"Test Error = %s\" % (1-accuracy_lr))\n",
    "print (\"Precision = %s\" %precision_lr)\n",
    "print (\"Recall = %s\" %recall_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Print the confusion matrix\n",
    "metrics_lr.confusionMatrix().toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bin_lr=BinaryClassificationMetrics(predictionAndLabels_lr)\n",
    "\n",
    "# Area under precision-recall curve\n",
    "print(\"Area under PR = %s\" % bin_lr.areaUnderPR)\n",
    "# Area under precision-recall curve\n",
    "print(\"Area under ROC = %s\" % bin_lr.areaUnderROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 with Spark 1.6",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "name": "machineLearningHAVCBluemix.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
